------------train
------------train
mynet(
  (context_path): MobileNetV3_Large(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (hs1): hswish()
    (bneck): Sequential(
      (0): Block(
        (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (1): Block(
        (conv1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (2): Block(
        (conv1): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
        (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (3): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
        (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (4): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
        (bn2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (5): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
        (bn2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (6): Block(
        (conv1): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (7): Block(
        (conv1): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
        (bn2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (8): Block(
        (conv1): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
        (bn2): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (9): Block(
        (conv1): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
        (bn2): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (10): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(112, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(28, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential(
          (0): Conv2d(80, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(112, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(28, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (12): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)
        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential(
          (0): Conv2d(112, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(160, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (14): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
  )
  (global_context): GlobalContext(
    (global_avg): AdaptiveAvgPool2d(output_size=1)
    (conv_last): ConvBnRelu(
      (conv): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bnrelu): InPlaceABNSync(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
    )
    (se_channel_att): ATT(
      (avg): AdaptiveAvgPool2d(output_size=1)
      (amg): AdaptiveMaxPool2d(output_size=1)
      (conv1): ConvBnRelu(
        (conv): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bnrelu): InPlaceABNSync(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
      )
      (conv2): ConvBnRelu(
        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): InPlaceABNSync(128, eps=1e-05, momentum=0.1, affine=True, activation=none)
      )
      (sigmoid): Sigmoid()
    )
  )
  (keep_res): KeepRes(
    (block1): sdBlock(
      (conv): ConvBnRelu(
        (conv): Conv2d(168, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bnrelu): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
      )
      (bn1): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
      (bn2): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=none)
      (relu): LeakyReLU(negative_slope=0.01)
      (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
      (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
      (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
    )
    (block2): sdBlock(
      (conv): ConvBnRelu(
        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bnrelu): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
      )
      (bn1): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
      (bn2): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=none)
      (relu): LeakyReLU(negative_slope=0.01)
      (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
      (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
      (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
    )
    (dim_reduction): ConvBnRelu(
      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bnrelu): InPlaceABNSync(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
    )
  )
  (merge_att): MergeAttention(
    (merge_att): ConvBnRelu(
      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bnrelu): InPlaceABNSync(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
    )
    (ch_avg_pool): AdaptiveAvgPool2d(output_size=1)
    (channel_se): Sequential(
      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): LeakyReLU(negative_slope=0.01)
      (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (3): InPlaceABNSync(32, eps=1e-05, momentum=0.1, affine=True, activation=none)
      (4): Sigmoid()
    )
    (dim_reduction): ConvBnRelu(
      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bnrelu): InPlaceABNSync(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
    )
  )
  (last_conv1): OutputLayer(
    (drop): Dropout2d(p=0.1)
    (conv): ConvBnRelu(
      (conv): Conv2d(32, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): InPlaceABNSync(19, eps=1e-05, momentum=0.1, affine=True, activation=none)
    )
  )
  (last_conv2): OutputLayer(
    (drop): Dropout2d(p=0.1)
    (conv): ConvBnRelu(
      (conv): Conv2d(32, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): InPlaceABNSync(19, eps=1e-05, momentum=0.1, affine=True, activation=none)
    )
  )
  (last_conv3): OutputLayer(
    (drop): Dropout2d(p=0.1)
    (conv): ConvBnRelu(
      (conv): Conv2d(32, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): InPlaceABNSync(19, eps=1e-05, momentum=0.1, affine=True, activation=none)
    )
  )
)
0
mynet(
  (context_path): MobileNetV3_Large(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (hs1): hswish()
    (bneck): Sequential(
      (0): Block(
        (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (1): Block(
        (conv1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (2): Block(
        (conv1): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
        (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (3): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
        (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (4): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
        (bn2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (5): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): ReLU(inplace)
        (conv2): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
        (bn2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): ReLU(inplace)
        (conv3): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (6): Block(
        (conv1): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (7): Block(
        (conv1): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
        (bn2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (8): Block(
        (conv1): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
        (bn2): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (9): Block(
        (conv1): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
        (bn2): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (10): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(112, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(28, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential(
          (0): Conv2d(80, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(112, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(28, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (12): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)
        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential(
          (0): Conv2d(112, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(160, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (14): Block(
        (se): SeModule(
          (se): Sequential(
            (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace)
            (3): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): hsigmoid()
          )
        )
        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear1): hswish()
        (conv2): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (nolinear2): hswish()
        (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
  )
  (global_context): GlobalContext(
    (global_avg): AdaptiveAvgPool2d(output_size=1)
    (conv_last): ConvBnRelu(
      (conv): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bnrelu): InPlaceABNSync(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
    )
    (se_channel_att): ATT(
      (avg): AdaptiveAvgPool2d(output_size=1)
      (amg): AdaptiveMaxPool2d(output_size=1)
      (conv1): ConvBnRelu(
        (conv): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bnrelu): InPlaceABNSync(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
      )
      (conv2): ConvBnRelu(
        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): InPlaceABNSync(128, eps=1e-05, momentum=0.1, affine=True, activation=none)
      )
      (sigmoid): Sigmoid()
    )
  )
  (keep_res): KeepRes(
    (block1): sdBlock(
      (conv): ConvBnRelu(
        (conv): Conv2d(168, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bnrelu): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
      )
      (bn1): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
      (bn2): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=none)
      (relu): LeakyReLU(negative_slope=0.01)
      (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
      (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
      (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
    )
    (block2): sdBlock(
      (conv): ConvBnRelu(
        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bnrelu): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
      )
      (bn1): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
      (bn2): InPlaceABNSync(64, eps=1e-05, momentum=0.1, affine=True, activation=none)
      (relu): LeakyReLU(negative_slope=0.01)
      (conv3x1_1): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
      (conv1x3_1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (conv3x1_2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
      (conv1x3_2): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
    )
    (dim_reduction): ConvBnRelu(
      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bnrelu): InPlaceABNSync(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
    )
  )
  (merge_att): MergeAttention(
    (merge_att): ConvBnRelu(
      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bnrelu): InPlaceABNSync(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
    )
    (ch_avg_pool): AdaptiveAvgPool2d(output_size=1)
    (channel_se): Sequential(
      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): LeakyReLU(negative_slope=0.01)
      (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (3): InPlaceABNSync(32, eps=1e-05, momentum=0.1, affine=True, activation=none)
      (4): Sigmoid()
    )
    (dim_reduction): ConvBnRelu(
      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bnrelu): InPlaceABNSync(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu, slope=0.01)
    )
  )
  (last_conv1): OutputLayer(
    (drop): Dropout2d(p=0.1)
    (conv): ConvBnRelu(
      (conv): Conv2d(32, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): InPlaceABNSync(19, eps=1e-05, momentum=0.1, affine=True, activation=none)
    )
  )
  (last_conv2): OutputLayer(
    (drop): Dropout2d(p=0.1)
    (conv): ConvBnRelu(
      (conv): Conv2d(32, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): InPlaceABNSync(19, eps=1e-05, momentum=0.1, affine=True, activation=none)
    )
  )
  (last_conv3): OutputLayer(
    (drop): Dropout2d(p=0.1)
    (conv): ConvBnRelu(
      (conv): Conv2d(32, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): InPlaceABNSync(19, eps=1e-05, momentum=0.1, affine=True, activation=none)
    )
  )
)
1
it: 100/80000, lr: 0.000020, loss: 9.8665, eta: 12:21:16, time: 55.1084
it: 200/80000, lr: 0.000040, loss: 9.4105, eta: 11:13:41, time: 45.6903
it: 300/80000, lr: 0.000079, loss: 8.6301, eta: 10:50:24, time: 45.6016
it: 400/80000, lr: 0.000157, loss: 7.4040, eta: 11:04:09, time: 53.3433
it: 500/80000, lr: 0.000314, loss: 5.9108, eta: 10:51:42, time: 45.6917
it: 600/80000, lr: 0.000627, loss: 4.9658, eta: 10:44:07, time: 46.1225
it: 700/80000, lr: 0.001250, loss: 4.8009, eta: 10:37:03, time: 45.3657
it: 800/80000, lr: 0.002495, loss: 4.3026, eta: 10:43:51, time: 52.7980
it: 900/80000, lr: 0.004977, loss: 4.0928, eta: 10:37:40, time: 45.1208
it: 1000/80000, lr: 0.009931, loss: 4.0625, eta: 10:32:56, time: 45.3866
==> warmup done, start to implement poly lr strategy
==> warmup done, start to implement poly lr strategy
it: 1100/80000, lr: 0.009989, loss: 3.9914, eta: 10:28:16, time: 44.8370
it: 1200/80000, lr: 0.009977, loss: 3.9575, eta: 10:32:08, time: 52.0305
it: 1300/80000, lr: 0.009966, loss: 3.7515, eta: 10:28:08, time: 44.9739
it: 1400/80000, lr: 0.009955, loss: 3.8084, eta: 10:24:23, time: 44.7281
it: 1500/80000, lr: 0.009943, loss: 3.8576, eta: 10:27:17, time: 51.9005
it: 1600/80000, lr: 0.009932, loss: 3.8491, eta: 10:23:42, time: 44.5455
it: 1700/80000, lr: 0.009920, loss: 3.6793, eta: 10:20:47, time: 44.9538
it: 1800/80000, lr: 0.009909, loss: 3.6862, eta: 10:18:14, time: 45.1599
it: 1900/80000, lr: 0.009898, loss: 3.6625, eta: 10:21:04, time: 52.7129
it: 2000/80000, lr: 0.009886, loss: 3.6489, eta: 10:18:28, time: 44.9520
it: 2100/80000, lr: 0.009875, loss: 3.8065, eta: 10:16:12, time: 45.1885
it: 2200/80000, lr: 0.009863, loss: 3.5324, eta: 10:14:07, time: 45.2488
it: 2300/80000, lr: 0.009852, loss: 3.7642, eta: 10:16:41, time: 53.3190
it: 2400/80000, lr: 0.009840, loss: 3.6958, eta: 10:14:49, time: 45.6281
it: 2500/80000, lr: 0.009829, loss: 3.6574, eta: 10:12:55, time: 45.4146
it: 2600/80000, lr: 0.009818, loss: 3.5684, eta: 10:10:51, time: 44.8661
it: 2700/80000, lr: 0.009806, loss: 3.6102, eta: 10:12:43, time: 52.9301
it: 2800/80000, lr: 0.009795, loss: 3.5403, eta: 10:10:48, time: 45.1254
it: 2900/80000, lr: 0.009783, loss: 3.5957, eta: 10:08:58, time: 45.1235
it: 3000/80000, lr: 0.009772, loss: 3.4605, eta: 10:10:37, time: 53.0892
it: 3100/80000, lr: 0.009761, loss: 3.6716, eta: 10:09:08, time: 45.8877
it: 3200/80000, lr: 0.009749, loss: 3.5623, eta: 10:07:27, time: 45.3149
it: 3300/80000, lr: 0.009738, loss: 3.5424, eta: 10:05:55, time: 45.5517
it: 3400/80000, lr: 0.009726, loss: 3.5038, eta: 10:07:08, time: 52.7039
it: 3500/80000, lr: 0.009715, loss: 3.5280, eta: 10:05:27, time: 45.1471
it: 3600/80000, lr: 0.009703, loss: 3.4440, eta: 10:03:47, time: 45.0169
it: 3700/80000, lr: 0.009692, loss: 3.4479, eta: 10:02:10, time: 45.0175
it: 3800/80000, lr: 0.009681, loss: 3.4212, eta: 10:03:17, time: 53.0513
it: 3900/80000, lr: 0.009669, loss: 3.6105, eta: 10:01:49, time: 45.4319
it: 4000/80000, lr: 0.009658, loss: 3.4958, eta: 10:00:23, time: 45.3843
it: 4100/80000, lr: 0.009646, loss: 3.4600, eta: 10:01:05, time: 52.2126
it: 4200/80000, lr: 0.009635, loss: 3.5107, eta: 9:59:33, time: 45.0911
it: 4300/80000, lr: 0.009623, loss: 3.5584, eta: 9:58:06, time: 45.2051
it: 4400/80000, lr: 0.009612, loss: 3.6157, eta: 9:56:45, time: 45.4280
it: 4500/80000, lr: 0.009600, loss: 3.3863, eta: 9:57:33, time: 53.0594
it: 4600/80000, lr: 0.009589, loss: 3.6339, eta: 9:56:12, time: 45.4635
it: 4700/80000, lr: 0.009578, loss: 3.4499, eta: 9:54:50, time: 45.2459
it: 4800/80000, lr: 0.009566, loss: 3.5079, eta: 9:53:31, time: 45.4045
it: 4900/80000, lr: 0.009555, loss: 3.4653, eta: 9:54:17, time: 53.4157
it: 5000/80000, lr: 0.009543, loss: 3.5292, eta: 9:52:54, time: 45.1079
it: 5100/80000, lr: 0.009532, loss: 3.5773, eta: 9:51:41, time: 45.7434
it: 5200/80000, lr: 0.009520, loss: 3.5605, eta: 9:50:22, time: 45.2112
it: 5300/80000, lr: 0.009509, loss: 3.5199, eta: 9:50:59, time: 53.2862
it: 5400/80000, lr: 0.009497, loss: 3.5183, eta: 9:49:45, time: 45.5456
it: 5500/80000, lr: 0.009486, loss: 3.4006, eta: 9:48:31, time: 45.5317
it: 5600/80000, lr: 0.009475, loss: 3.4400, eta: 9:48:58, time: 52.9784
it: 5700/80000, lr: 0.009463, loss: 3.4338, eta: 9:47:34, time: 44.7139
it: 5800/80000, lr: 0.009452, loss: 3.4464, eta: 9:46:15, time: 44.9674
it: 5900/80000, lr: 0.009440, loss: 3.3877, eta: 9:45:00, time: 45.1889
it: 6000/80000, lr: 0.009429, loss: 3.4110, eta: 9:45:22, time: 52.9760
it: 6100/80000, lr: 0.009417, loss: 3.4580, eta: 9:44:17, time: 46.0673
it: 6200/80000, lr: 0.009406, loss: 3.4432, eta: 9:43:12, time: 45.9593
it: 6300/80000, lr: 0.009394, loss: 3.4239, eta: 9:42:09, time: 46.0452
it: 6400/80000, lr: 0.009383, loss: 3.4625, eta: 9:42:34, time: 53.6972
it: 6500/80000, lr: 0.009371, loss: 3.4526, eta: 9:41:29, time: 45.9392
it: 6600/80000, lr: 0.009360, loss: 3.2946, eta: 9:40:26, time: 46.0565
it: 6700/80000, lr: 0.009348, loss: 3.4473, eta: 9:40:44, time: 53.4785
it: 6800/80000, lr: 0.009337, loss: 3.6094, eta: 9:39:37, time: 45.7122
it: 6900/80000, lr: 0.009325, loss: 3.4347, eta: 9:38:32, time: 45.7945
it: 7000/80000, lr: 0.009314, loss: 3.4470, eta: 9:37:28, time: 45.9717
it: 7100/80000, lr: 0.009302, loss: 3.4409, eta: 9:37:41, time: 53.3280
it: 7200/80000, lr: 0.009291, loss: 3.4324, eta: 9:36:31, time: 45.3165
it: 7300/80000, lr: 0.009279, loss: 3.3856, eta: 9:35:28, time: 45.9502
it: 7400/80000, lr: 0.009268, loss: 3.3442, eta: 9:34:24, time: 45.8620
it: 7500/80000, lr: 0.009256, loss: 3.3763, eta: 9:34:28, time: 52.7132
it: 7600/80000, lr: 0.009245, loss: 3.4217, eta: 9:33:26, time: 46.0271
it: 7700/80000, lr: 0.009233, loss: 3.4214, eta: 9:32:22, time: 45.8005
it: 7800/80000, lr: 0.009222, loss: 3.3597, eta: 9:31:16, time: 45.5643
it: 7900/80000, lr: 0.009210, loss: 3.4759, eta: 9:31:28, time: 53.9268
it: 8000/80000, lr: 0.009199, loss: 3.4988, eta: 9:30:25, time: 45.8652
it: 8100/80000, lr: 0.009187, loss: 3.3626, eta: 9:29:26, time: 46.2530
it: 8200/80000, lr: 0.009176, loss: 3.3764, eta: 9:29:30, time: 53.4268
it: 8300/80000, lr: 0.009164, loss: 3.2438, eta: 9:28:26, time: 45.5916
it: 8400/80000, lr: 0.009153, loss: 3.4108, eta: 9:27:22, time: 45.7164
it: 8500/80000, lr: 0.009141, loss: 3.3997, eta: 9:26:19, time: 45.6892
it: 8600/80000, lr: 0.009130, loss: 3.5770, eta: 9:26:18, time: 53.0940
it: 8700/80000, lr: 0.009118, loss: 3.4692, eta: 9:25:16, time: 45.8513
it: 8800/80000, lr: 0.009107, loss: 3.3957, eta: 9:24:16, time: 46.0397
it: 8900/80000, lr: 0.009095, loss: 3.3904, eta: 9:23:15, time: 45.9077
it: 9000/80000, lr: 0.009084, loss: 3.3774, eta: 9:23:16, time: 53.6906
it: 9100/80000, lr: 0.009072, loss: 3.3099, eta: 9:22:17, time: 46.0396
it: 9200/80000, lr: 0.009061, loss: 3.3176, eta: 9:21:19, time: 46.3077
it: 9300/80000, lr: 0.009049, loss: 3.3538, eta: 9:20:16, time: 45.4866
it: 9400/80000, lr: 0.009038, loss: 3.2741, eta: 9:20:11, time: 53.2587
it: 9500/80000, lr: 0.009026, loss: 3.3195, eta: 9:19:09, time: 45.6772
it: 9600/80000, lr: 0.009015, loss: 3.4304, eta: 9:18:08, time: 45.6949
it: 9700/80000, lr: 0.009003, loss: 3.2702, eta: 9:18:04, time: 53.5835
it: 9800/80000, lr: 0.008992, loss: 3.3244, eta: 9:17:07, time: 46.3913
it: 9900/80000, lr: 0.008980, loss: 3.5121, eta: 9:16:12, time: 46.5182
it: 10000/80000, lr: 0.008969, loss: 3.4005, eta: 9:15:19, time: 46.7871
it: 10100/80000, lr: 0.008957, loss: 3.4461, eta: 9:15:16, time: 54.1068
it: 10200/80000, lr: 0.008946, loss: 3.3070, eta: 9:14:20, time: 46.4447
it: 10300/80000, lr: 0.008934, loss: 3.5180, eta: 9:13:26, time: 46.6793
it: 10400/80000, lr: 0.008923, loss: 3.4138, eta: 9:12:30, time: 46.3571
it: 10500/80000, lr: 0.008911, loss: 3.4486, eta: 9:12:19, time: 53.1806
it: 10600/80000, lr: 0.008899, loss: 3.4516, eta: 9:11:20, time: 46.0500
it: 10700/80000, lr: 0.008888, loss: 3.3331, eta: 9:10:22, time: 46.0636
it: 10800/80000, lr: 0.008876, loss: 3.3842, eta: 9:10:10, time: 53.1200
it: 10900/80000, lr: 0.008865, loss: 3.4286, eta: 9:09:09, time: 45.5991
it: 11000/80000, lr: 0.008853, loss: 3.3864, eta: 9:08:09, time: 45.8431
it: 11100/80000, lr: 0.008842, loss: 3.2931, eta: 9:07:09, time: 45.6216
it: 11200/80000, lr: 0.008830, loss: 3.3100, eta: 9:06:53, time: 52.8675
it: 11300/80000, lr: 0.008819, loss: 3.4373, eta: 9:05:54, time: 45.8137
it: 11400/80000, lr: 0.008807, loss: 3.2796, eta: 9:04:56, time: 45.8810
it: 11500/80000, lr: 0.008796, loss: 3.3479, eta: 9:03:58, time: 46.0109
it: 11600/80000, lr: 0.008784, loss: 3.3143, eta: 9:03:39, time: 52.4748
it: 11700/80000, lr: 0.008772, loss: 3.2803, eta: 9:02:42, time: 46.0706
it: 11800/80000, lr: 0.008761, loss: 3.3324, eta: 9:01:45, time: 46.1054
it: 11900/80000, lr: 0.008749, loss: 3.3660, eta: 9:00:45, time: 45.5411
it: 12000/80000, lr: 0.008738, loss: 3.3251, eta: 9:00:28, time: 53.0029
it: 12100/80000, lr: 0.008726, loss: 3.3276, eta: 8:59:31, time: 45.9601
it: 12200/80000, lr: 0.008715, loss: 3.3869, eta: 8:58:36, time: 46.3481
it: 12300/80000, lr: 0.008703, loss: 3.2443, eta: 8:58:21, time: 53.7033
it: 12400/80000, lr: 0.008691, loss: 3.3578, eta: 8:57:26, time: 46.3113
it: 12500/80000, lr: 0.008680, loss: 3.3501, eta: 8:56:31, time: 46.3286
it: 12600/80000, lr: 0.008668, loss: 3.3809, eta: 8:55:35, time: 46.2180
it: 12700/80000, lr: 0.008657, loss: 3.3324, eta: 8:55:17, time: 53.2235
it: 12800/80000, lr: 0.008645, loss: 3.3637, eta: 8:54:18, time: 45.6388
it: 12900/80000, lr: 0.008634, loss: 3.3772, eta: 8:53:21, time: 45.8299
it: 13000/80000, lr: 0.008622, loss: 3.3179, eta: 8:52:24, time: 45.8480
it: 13100/80000, lr: 0.008610, loss: 3.3036, eta: 8:52:07, time: 53.7093
it: 13200/80000, lr: 0.008599, loss: 3.3179, eta: 8:51:11, time: 46.2034
it: 13300/80000, lr: 0.008587, loss: 3.2748, eta: 8:50:17, time: 46.3138
it: 13400/80000, lr: 0.008576, loss: 3.2792, eta: 8:50:02, time: 54.2421
it: 13500/80000, lr: 0.008564, loss: 3.3572, eta: 8:49:04, time: 45.7430
it: 13600/80000, lr: 0.008552, loss: 3.2083, eta: 8:48:09, time: 46.1706
it: 13700/80000, lr: 0.008541, loss: 3.3496, eta: 8:47:14, time: 46.2857
it: 13800/80000, lr: 0.008529, loss: 3.3554, eta: 8:46:55, time: 53.7974
it: 13900/80000, lr: 0.008518, loss: 3.3539, eta: 8:46:00, time: 46.2731
it: 14000/80000, lr: 0.008506, loss: 3.3197, eta: 8:45:05, time: 46.1407
it: 14100/80000, lr: 0.008495, loss: 3.3804, eta: 8:44:12, time: 46.5179
it: 14200/80000, lr: 0.008483, loss: 3.2959, eta: 8:43:50, time: 53.2269
it: 14300/80000, lr: 0.008471, loss: 3.3217, eta: 8:42:55, time: 46.2681
it: 14400/80000, lr: 0.008460, loss: 3.3168, eta: 8:42:02, time: 46.5290
it: 14500/80000, lr: 0.008448, loss: 3.4192, eta: 8:41:05, time: 45.8488
it: 14600/80000, lr: 0.008436, loss: 3.4433, eta: 8:40:45, time: 53.9621
it: 14700/80000, lr: 0.008425, loss: 3.2964, eta: 8:39:50, time: 46.1305
it: 14800/80000, lr: 0.008413, loss: 3.2881, eta: 8:38:55, time: 46.0547
it: 14900/80000, lr: 0.008402, loss: 3.3200, eta: 8:38:34, time: 53.9155
it: 15000/80000, lr: 0.008390, loss: 3.3100, eta: 8:37:41, time: 46.6467
it: 15100/80000, lr: 0.008378, loss: 3.3208, eta: 8:36:49, time: 46.7233
it: 15200/80000, lr: 0.008367, loss: 3.2835, eta: 8:35:56, time: 46.4288
it: 15300/80000, lr: 0.008355, loss: 3.2426, eta: 8:35:33, time: 53.7979
it: 15400/80000, lr: 0.008344, loss: 3.3008, eta: 8:34:39, time: 46.1869
it: 15500/80000, lr: 0.008332, loss: 3.2264, eta: 8:33:44, time: 46.1444
it: 15600/80000, lr: 0.008320, loss: 3.2775, eta: 8:32:49, time: 46.1416
it: 15700/80000, lr: 0.008309, loss: 3.2006, eta: 8:32:23, time: 52.9189
it: 15800/80000, lr: 0.008297, loss: 3.3103, eta: 8:31:25, time: 45.3354
it: 15900/80000, lr: 0.008285, loss: 3.2516, eta: 8:30:27, time: 45.2895
it: 16000/80000, lr: 0.008274, loss: 3.2436, eta: 8:29:57, time: 52.1764
it: 16100/80000, lr: 0.008262, loss: 3.3289, eta: 8:29:01, time: 45.8125
it: 16200/80000, lr: 0.008251, loss: 3.2816, eta: 8:28:05, time: 45.7690
it: 16300/80000, lr: 0.008239, loss: 3.1926, eta: 8:27:10, time: 45.9616
it: 16400/80000, lr: 0.008227, loss: 3.2685, eta: 8:26:40, time: 52.3889
it: 16500/80000, lr: 0.008216, loss: 3.2933, eta: 8:25:42, time: 44.9659
it: 16600/80000, lr: 0.008204, loss: 3.3154, eta: 8:24:44, time: 45.3070
it: 16700/80000, lr: 0.008192, loss: 3.2957, eta: 8:23:47, time: 45.2272
it: 16800/80000, lr: 0.008181, loss: 3.3220, eta: 8:23:18, time: 52.8070
it: 16900/80000, lr: 0.008169, loss: 3.3504, eta: 8:22:22, time: 45.3825
it: 17000/80000, lr: 0.008157, loss: 3.2711, eta: 8:21:26, time: 45.6374
it: 17100/80000, lr: 0.008146, loss: 3.2629, eta: 8:20:29, time: 45.2369
it: 17200/80000, lr: 0.008134, loss: 3.3127, eta: 8:19:59, time: 52.7389
it: 17300/80000, lr: 0.008122, loss: 3.3102, eta: 8:19:03, time: 45.4973
it: 17400/80000, lr: 0.008111, loss: 3.2386, eta: 8:18:07, time: 45.4882
it: 17500/80000, lr: 0.008099, loss: 3.2664, eta: 8:17:39, time: 53.2074
it: 17600/80000, lr: 0.008087, loss: 3.2020, eta: 8:16:45, time: 45.9342
it: 17700/80000, lr: 0.008076, loss: 3.2689, eta: 8:15:53, time: 46.4401
it: 17800/80000, lr: 0.008064, loss: 3.3114, eta: 8:14:59, time: 46.2365
it: 17900/80000, lr: 0.008052, loss: 3.3479, eta: 8:14:30, time: 53.0366
it: 18000/80000, lr: 0.008041, loss: 3.2555, eta: 8:13:35, time: 45.7311
it: 18100/80000, lr: 0.008029, loss: 3.3488, eta: 8:12:41, time: 45.8208
it: 18200/80000, lr: 0.008017, loss: 3.2877, eta: 8:11:46, time: 45.7429
it: 18300/80000, lr: 0.008006, loss: 3.3612, eta: 8:11:15, time: 52.5971
it: 18400/80000, lr: 0.007994, loss: 3.2295, eta: 8:10:21, time: 45.8237
it: 18500/80000, lr: 0.007982, loss: 3.2545, eta: 8:09:28, time: 46.2111
it: 18600/80000, lr: 0.007971, loss: 3.2566, eta: 8:08:33, time: 45.5345
it: 18700/80000, lr: 0.007959, loss: 3.3861, eta: 8:08:05, time: 53.7602
it: 18800/80000, lr: 0.007947, loss: 3.2495, eta: 8:07:14, time: 47.0025
it: 18900/80000, lr: 0.007936, loss: 3.1885, eta: 8:06:22, time: 46.4325
it: 19000/80000, lr: 0.007924, loss: 3.4460, eta: 8:05:53, time: 53.6764
it: 19100/80000, lr: 0.007912, loss: 3.2319, eta: 8:04:58, time: 45.4838
it: 19200/80000, lr: 0.007901, loss: 3.2399, eta: 8:04:05, time: 46.0583
it: 19300/80000, lr: 0.007889, loss: 3.2898, eta: 8:03:12, time: 46.0414
it: 19400/80000, lr: 0.007877, loss: 3.3143, eta: 8:02:43, time: 53.8630
it: 19500/80000, lr: 0.007865, loss: 3.2883, eta: 8:01:51, time: 46.5199
it: 19600/80000, lr: 0.007854, loss: 3.3807, eta: 8:01:01, time: 46.8745
it: 19700/80000, lr: 0.007842, loss: 3.1672, eta: 8:00:08, time: 46.3107
it: 19800/80000, lr: 0.007830, loss: 3.2755, eta: 7:59:37, time: 53.1856
it: 19900/80000, lr: 0.007819, loss: 3.2467, eta: 7:58:44, time: 46.0747
it: 20000/80000, lr: 0.007807, loss: 3.2340, eta: 7:57:51, time: 46.0590
it: 20100/80000, lr: 0.007795, loss: 3.4119, eta: 7:57:19, time: 52.9505
it: 20200/80000, lr: 0.007783, loss: 3.2720, eta: 7:56:28, time: 46.8467
it: 20300/80000, lr: 0.007772, loss: 3.1486, eta: 7:55:37, time: 46.7442
it: 20400/80000, lr: 0.007760, loss: 3.4002, eta: 7:54:45, time: 46.4662
it: 20500/80000, lr: 0.007748, loss: 3.4117, eta: 7:54:16, time: 54.2047
it: 20600/80000, lr: 0.007737, loss: 3.3161, eta: 7:53:25, time: 46.7962
it: 20700/80000, lr: 0.007725, loss: 3.3266, eta: 7:52:35, time: 46.8563
it: 20800/80000, lr: 0.007713, loss: 3.2628, eta: 7:51:45, time: 47.0011
it: 20900/80000, lr: 0.007701, loss: 3.2673, eta: 7:51:16, time: 54.4933
it: 21000/80000, lr: 0.007690, loss: 3.2230, eta: 7:50:24, time: 46.5663
it: 21100/80000, lr: 0.007678, loss: 3.2792, eta: 7:49:33, time: 46.5470
it: 21200/80000, lr: 0.007666, loss: 3.2301, eta: 7:48:40, time: 46.0341
it: 21300/80000, lr: 0.007654, loss: 3.2765, eta: 7:48:07, time: 53.2116
it: 21400/80000, lr: 0.007643, loss: 3.3116, eta: 7:47:14, time: 46.0987
it: 21500/80000, lr: 0.007631, loss: 3.3358, eta: 7:46:22, time: 45.9579
it: 21600/80000, lr: 0.007619, loss: 3.4211, eta: 7:45:51, time: 54.0946
it: 21700/80000, lr: 0.007608, loss: 3.2002, eta: 7:44:58, time: 46.2594
it: 21800/80000, lr: 0.007596, loss: 3.2420, eta: 7:44:07, time: 46.5828
it: 21900/80000, lr: 0.007584, loss: 3.2404, eta: 7:43:16, time: 46.7231
it: 22000/80000, lr: 0.007572, loss: 3.1410, eta: 7:42:43, time: 53.2682
it: 22100/80000, lr: 0.007561, loss: 3.3350, eta: 7:41:51, time: 46.2889
it: 22200/80000, lr: 0.007549, loss: 3.1738, eta: 7:40:59, time: 46.1562
it: 22300/80000, lr: 0.007537, loss: 3.2710, eta: 7:40:06, time: 45.9564
it: 22400/80000, lr: 0.007525, loss: 3.2216, eta: 7:39:36, time: 54.9614
it: 22500/80000, lr: 0.007514, loss: 3.3514, eta: 7:38:46, time: 46.8896
it: 22600/80000, lr: 0.007502, loss: 3.2799, eta: 7:37:55, time: 46.8755
it: 22700/80000, lr: 0.007490, loss: 3.2639, eta: 7:37:23, time: 54.0731
it: 22800/80000, lr: 0.007478, loss: 3.1798, eta: 7:36:30, time: 45.7767
it: 22900/80000, lr: 0.007466, loss: 3.1774, eta: 7:35:38, time: 46.2298
it: 23000/80000, lr: 0.007455, loss: 3.1951, eta: 7:34:46, time: 46.1485
it: 23100/80000, lr: 0.007443, loss: 3.1640, eta: 7:34:12, time: 53.5469
it: 23200/80000, lr: 0.007431, loss: 3.4041, eta: 7:33:20, time: 46.0810
it: 23300/80000, lr: 0.007419, loss: 3.2023, eta: 7:32:28, time: 46.4345
it: 23400/80000, lr: 0.007408, loss: 3.2552, eta: 7:31:39, time: 47.1603
it: 23500/80000, lr: 0.007396, loss: 3.2127, eta: 7:31:07, time: 54.6749
it: 23600/80000, lr: 0.007384, loss: 3.2658, eta: 7:30:17, time: 46.9965
it: 23700/80000, lr: 0.007372, loss: 3.2233, eta: 7:29:27, time: 47.1327
it: 23800/80000, lr: 0.007360, loss: 3.2404, eta: 7:28:36, time: 46.4749
it: 23900/80000, lr: 0.007349, loss: 3.1995, eta: 7:28:07, time: 56.0007
it: 24000/80000, lr: 0.007337, loss: 3.1921, eta: 7:27:16, time: 46.7530
it: 24100/80000, lr: 0.007325, loss: 3.2887, eta: 7:26:26, time: 46.9863
it: 24200/80000, lr: 0.007313, loss: 3.3657, eta: 7:25:54, time: 54.4989
it: 24300/80000, lr: 0.007302, loss: 3.2278, eta: 7:25:01, time: 46.0813
it: 24400/80000, lr: 0.007290, loss: 3.2359, eta: 7:24:10, time: 46.2894
it: 24500/80000, lr: 0.007278, loss: 3.2664, eta: 7:23:19, time: 46.8455
it: 24600/80000, lr: 0.007266, loss: 3.1239, eta: 7:22:47, time: 54.9718
it: 24700/80000, lr: 0.007254, loss: 3.2652, eta: 7:21:56, time: 46.6606
it: 24800/80000, lr: 0.007242, loss: 3.2252, eta: 7:21:05, time: 46.6006
it: 24900/80000, lr: 0.007231, loss: 3.2043, eta: 7:20:14, time: 46.2383
it: 25000/80000, lr: 0.007219, loss: 3.2902, eta: 7:19:42, time: 55.3617
it: 25100/80000, lr: 0.007207, loss: 3.2884, eta: 7:18:55, time: 48.3663
it: 25200/80000, lr: 0.007195, loss: 3.3011, eta: 7:18:12, time: 50.3407
it: 25300/80000, lr: 0.007183, loss: 3.2520, eta: 7:17:41, time: 55.4960
it: 25400/80000, lr: 0.007172, loss: 3.2415, eta: 7:16:47, time: 45.3588
it: 25500/80000, lr: 0.007160, loss: 3.1989, eta: 7:15:55, time: 46.1444
it: 25600/80000, lr: 0.007148, loss: 3.2157, eta: 7:15:03, time: 46.0103
it: 25700/80000, lr: 0.007136, loss: 3.2623, eta: 7:14:26, time: 53.3718
it: 25800/80000, lr: 0.007124, loss: 3.1905, eta: 7:13:34, time: 46.2555
it: 25900/80000, lr: 0.007112, loss: 3.2362, eta: 7:12:43, time: 46.5266
it: 26000/80000, lr: 0.007101, loss: 3.2542, eta: 7:11:52, time: 46.5509
it: 26100/80000, lr: 0.007089, loss: 3.2894, eta: 7:11:15, time: 53.1983
it: 26200/80000, lr: 0.007077, loss: 3.2099, eta: 7:10:23, time: 46.2457
it: 26300/80000, lr: 0.007065, loss: 3.2275, eta: 7:09:32, time: 46.1483
it: 26400/80000, lr: 0.007053, loss: 3.2729, eta: 7:08:40, time: 45.9691
it: 26500/80000, lr: 0.007041, loss: 3.2112, eta: 7:08:03, time: 53.6979
it: 26600/80000, lr: 0.007030, loss: 3.3216, eta: 7:07:12, time: 46.2896
it: 26700/80000, lr: 0.007018, loss: 3.1522, eta: 7:06:21, time: 46.5125
it: 26800/80000, lr: 0.007006, loss: 3.2363, eta: 7:05:46, time: 54.4672
it: 26900/80000, lr: 0.006994, loss: 3.2474, eta: 7:04:54, time: 46.2711
it: 27000/80000, lr: 0.006982, loss: 3.2093, eta: 7:04:02, time: 46.1434
it: 27100/80000, lr: 0.006970, loss: 3.2728, eta: 7:03:59, time: 70.9048
it: 27200/80000, lr: 0.006958, loss: 3.1312, eta: 7:03:49, time: 67.4721
it: 27300/80000, lr: 0.006947, loss: 3.0882, eta: 7:02:59, time: 47.5441
it: 27400/80000, lr: 0.006935, loss: 3.1873, eta: 7:02:11, time: 47.9059
it: 27500/80000, lr: 0.006923, loss: 3.1953, eta: 7:01:22, time: 47.6317
it: 27600/80000, lr: 0.006911, loss: 3.2379, eta: 7:00:47, time: 55.5270
it: 27700/80000, lr: 0.006899, loss: 3.2738, eta: 6:59:56, time: 46.4653
it: 27800/80000, lr: 0.006887, loss: 3.1666, eta: 6:59:44, time: 67.4622
it: 27900/80000, lr: 0.006875, loss: 3.2290, eta: 6:58:58, time: 49.5161
it: 28000/80000, lr: 0.006864, loss: 3.1821, eta: 6:58:21, time: 54.0373
it: 28100/80000, lr: 0.006852, loss: 3.1260, eta: 6:57:30, time: 47.1481
it: 28200/80000, lr: 0.006840, loss: 3.1333, eta: 6:56:38, time: 45.8444
it: 28300/80000, lr: 0.006828, loss: 3.2564, eta: 6:55:58, time: 52.8082
it: 28400/80000, lr: 0.006816, loss: 3.1387, eta: 6:55:04, time: 45.1027
it: 28500/80000, lr: 0.006804, loss: 3.3094, eta: 6:54:10, time: 45.4444
it: 28600/80000, lr: 0.006792, loss: 3.2062, eta: 6:53:17, time: 45.6381
it: 28700/80000, lr: 0.006780, loss: 3.2558, eta: 6:52:37, time: 52.5173
it: 28800/80000, lr: 0.006768, loss: 3.1631, eta: 6:51:43, time: 45.2412
it: 28900/80000, lr: 0.006757, loss: 3.2407, eta: 6:50:49, time: 45.0147
it: 29000/80000, lr: 0.006745, loss: 3.1562, eta: 6:49:55, time: 44.7316
it: 29100/80000, lr: 0.006733, loss: 3.2610, eta: 6:49:13, time: 51.9226
it: 29200/80000, lr: 0.006721, loss: 3.1435, eta: 6:48:19, time: 44.9727
it: 29300/80000, lr: 0.006709, loss: 3.3128, eta: 6:47:25, time: 45.1504
it: 29400/80000, lr: 0.006697, loss: 3.1493, eta: 6:46:47, time: 53.7787
it: 29500/80000, lr: 0.006685, loss: 3.2246, eta: 6:45:54, time: 45.3361
it: 29600/80000, lr: 0.006673, loss: 3.2949, eta: 6:45:01, time: 45.8199
it: 29700/80000, lr: 0.006661, loss: 3.2450, eta: 6:44:08, time: 45.2603
it: 29800/80000, lr: 0.006649, loss: 3.2513, eta: 6:43:27, time: 52.4818
it: 29900/80000, lr: 0.006637, loss: 3.1749, eta: 6:42:34, time: 45.2459
it: 30000/80000, lr: 0.006625, loss: 3.1466, eta: 6:41:41, time: 45.5445
it: 30100/80000, lr: 0.006614, loss: 3.2543, eta: 6:40:49, time: 45.5091
it: 30200/80000, lr: 0.006602, loss: 3.2521, eta: 6:40:08, time: 52.7748
it: 30300/80000, lr: 0.006590, loss: 3.1192, eta: 6:39:14, time: 44.9037
it: 30400/80000, lr: 0.006578, loss: 3.1844, eta: 6:38:21, time: 45.3154
it: 30500/80000, lr: 0.006566, loss: 3.2216, eta: 6:37:28, time: 44.8105
it: 30600/80000, lr: 0.006554, loss: 3.1958, eta: 6:37:12, time: 68.3850
it: 30700/80000, lr: 0.006542, loss: 3.3861, eta: 6:36:19, time: 45.4034
it: 30800/80000, lr: 0.006530, loss: 3.2125, eta: 6:35:28, time: 46.0052
it: 30900/80000, lr: 0.006518, loss: 3.2542, eta: 6:34:54, time: 57.4397
it: 31000/80000, lr: 0.006506, loss: 3.1397, eta: 6:34:15, time: 54.0655
it: 31100/80000, lr: 0.006494, loss: 3.1439, eta: 6:33:32, time: 51.9392
it: 31200/80000, lr: 0.006482, loss: 3.1512, eta: 6:32:50, time: 52.1741
it: 31300/80000, lr: 0.006470, loss: 3.2428, eta: 6:32:18, time: 58.4714
it: 31400/80000, lr: 0.006458, loss: 3.1845, eta: 6:31:32, time: 50.1734
it: 31500/80000, lr: 0.006446, loss: 3.2062, eta: 6:30:44, time: 48.5057
it: 31600/80000, lr: 0.006434, loss: 3.2920, eta: 6:30:00, time: 50.9360
it: 31700/80000, lr: 0.006422, loss: 3.2515, eta: 6:29:27, time: 58.6967
it: 31800/80000, lr: 0.006410, loss: 3.1773, eta: 6:28:38, time: 47.8206
it: 31900/80000, lr: 0.006398, loss: 3.2299, eta: 6:27:53, time: 50.5799
it: 32000/80000, lr: 0.006386, loss: 3.1466, eta: 6:27:25, time: 61.9453
it: 32100/80000, lr: 0.006374, loss: 3.1909, eta: 6:26:44, time: 53.4235
it: 32200/80000, lr: 0.006363, loss: 3.1996, eta: 6:25:58, time: 50.3465
it: 32300/80000, lr: 0.006351, loss: 3.2647, eta: 6:25:14, time: 51.3791
it: 32400/80000, lr: 0.006339, loss: 3.1935, eta: 6:24:45, time: 61.7155
it: 32500/80000, lr: 0.006327, loss: 3.2257, eta: 6:23:59, time: 50.3731
it: 32600/80000, lr: 0.006315, loss: 3.1037, eta: 6:23:12, time: 49.3055
it: 32700/80000, lr: 0.006303, loss: 3.1376, eta: 6:22:25, time: 49.3793
it: 32800/80000, lr: 0.006291, loss: 3.1784, eta: 6:21:57, time: 63.0139
it: 32900/80000, lr: 0.006279, loss: 3.1084, eta: 6:21:10, time: 49.6624
it: 33000/80000, lr: 0.006267, loss: 3.1601, eta: 6:20:20, time: 47.5196
it: 33100/80000, lr: 0.006255, loss: 3.2086, eta: 6:19:32, time: 48.9783
it: 33200/80000, lr: 0.006243, loss: 3.1360, eta: 6:19:06, time: 64.5108
it: 33300/80000, lr: 0.006231, loss: 3.1227, eta: 6:18:21, time: 51.2440
it: 33400/80000, lr: 0.006219, loss: 3.1810, eta: 6:17:35, time: 50.2594
it: 33500/80000, lr: 0.006207, loss: 3.1410, eta: 6:17:09, time: 65.0869
it: 33600/80000, lr: 0.006195, loss: 3.2643, eta: 6:16:26, time: 52.4492
it: 33700/80000, lr: 0.006183, loss: 3.1308, eta: 6:15:42, time: 52.4067
it: 33800/80000, lr: 0.006171, loss: 3.1216, eta: 6:15:05, time: 57.0471
it: 33900/80000, lr: 0.006158, loss: 3.2032, eta: 6:14:36, time: 62.8756
it: 34000/80000, lr: 0.006146, loss: 3.2130, eta: 6:13:49, time: 50.1549
it: 34100/80000, lr: 0.006134, loss: 3.1641, eta: 6:13:03, time: 50.9033
it: 34200/80000, lr: 0.006122, loss: 3.2248, eta: 6:12:18, time: 51.6788
it: 34300/80000, lr: 0.006110, loss: 3.2107, eta: 6:11:47, time: 61.9496
it: 34400/80000, lr: 0.006098, loss: 3.1334, eta: 6:11:01, time: 50.8722
it: 34500/80000, lr: 0.006086, loss: 3.1912, eta: 6:10:12, time: 48.7667
it: 34600/80000, lr: 0.006074, loss: 3.1821, eta: 6:09:39, time: 61.4163
it: 34700/80000, lr: 0.006062, loss: 3.1357, eta: 6:08:54, time: 51.4618
it: 34800/80000, lr: 0.006050, loss: 3.1335, eta: 6:08:03, time: 46.8941
it: 34900/80000, lr: 0.006038, loss: 3.1111, eta: 6:07:13, time: 48.6364
it: 35000/80000, lr: 0.006026, loss: 3.1628, eta: 6:06:41, time: 61.6609
it: 35100/80000, lr: 0.006014, loss: 3.1200, eta: 6:05:55, time: 50.8958
it: 35200/80000, lr: 0.006002, loss: 3.1252, eta: 6:05:07, time: 50.0629
it: 35300/80000, lr: 0.005990, loss: 3.1912, eta: 6:04:22, time: 51.3687
it: 35400/80000, lr: 0.005978, loss: 3.2472, eta: 6:03:45, time: 58.9948
it: 35500/80000, lr: 0.005966, loss: 3.1626, eta: 6:02:54, time: 46.6948
it: 35600/80000, lr: 0.005954, loss: 3.1855, eta: 6:02:03, time: 47.2659
it: 35700/80000, lr: 0.005942, loss: 3.1685, eta: 6:01:10, time: 45.5717
it: 35800/80000, lr: 0.005930, loss: 3.1976, eta: 6:00:29, time: 55.3221
it: 35900/80000, lr: 0.005918, loss: 3.1960, eta: 5:59:36, time: 45.9033
it: 36000/80000, lr: 0.005905, loss: 3.0796, eta: 5:58:44, time: 46.4784
it: 36100/80000, lr: 0.005893, loss: 3.1815, eta: 5:58:09, time: 60.2142
it: 36200/80000, lr: 0.005881, loss: 3.1913, eta: 5:57:17, time: 46.3286
it: 36300/80000, lr: 0.005869, loss: 3.1777, eta: 5:56:25, time: 46.8841
it: 36400/80000, lr: 0.005857, loss: 3.2525, eta: 5:55:35, time: 47.6722
it: 36500/80000, lr: 0.005845, loss: 3.1672, eta: 5:55:00, time: 60.7436
it: 36600/80000, lr: 0.005833, loss: 3.1519, eta: 5:54:09, time: 47.2363
it: 36700/80000, lr: 0.005821, loss: 3.1315, eta: 5:53:16, time: 45.7346
it: 36800/80000, lr: 0.005809, loss: 3.1701, eta: 5:52:25, time: 47.0771
it: 36900/80000, lr: 0.005797, loss: 3.1787, eta: 5:51:46, time: 57.0953
it: 37000/80000, lr: 0.005785, loss: 3.0944, eta: 5:50:52, time: 45.3433
it: 37100/80000, lr: 0.005772, loss: 3.1888, eta: 5:50:01, time: 47.1017
it: 37200/80000, lr: 0.005760, loss: 3.1925, eta: 5:49:10, time: 46.9145
it: 37300/80000, lr: 0.005748, loss: 3.1837, eta: 5:48:37, time: 62.8308
it: 37400/80000, lr: 0.005736, loss: 3.1706, eta: 5:47:47, time: 48.0682
it: 37500/80000, lr: 0.005724, loss: 3.0887, eta: 5:47:01, time: 51.7394
it: 37600/80000, lr: 0.005712, loss: 3.0779, eta: 5:46:22, time: 57.4100
it: 37700/80000, lr: 0.005700, loss: 3.1251, eta: 5:45:31, time: 47.3556
it: 37800/80000, lr: 0.005688, loss: 3.0864, eta: 5:44:46, time: 52.6918
it: 37900/80000, lr: 0.005675, loss: 3.1269, eta: 5:43:55, time: 47.4847
it: 38000/80000, lr: 0.005663, loss: 3.1679, eta: 5:43:15, time: 57.0568
it: 38100/80000, lr: 0.005651, loss: 3.1833, eta: 5:42:23, time: 46.7792
it: 38200/80000, lr: 0.005639, loss: 3.1525, eta: 5:41:31, time: 46.0115
it: 38300/80000, lr: 0.005627, loss: 3.2897, eta: 5:40:40, time: 46.7099
it: 38400/80000, lr: 0.005615, loss: 3.1119, eta: 5:40:01, time: 59.0608
it: 38500/80000, lr: 0.005603, loss: 3.2212, eta: 5:39:10, time: 47.0260
it: 38600/80000, lr: 0.005590, loss: 3.1422, eta: 5:38:18, time: 46.3969
it: 38700/80000, lr: 0.005578, loss: 3.1717, eta: 5:37:38, time: 57.3172
it: 38800/80000, lr: 0.005566, loss: 3.1817, eta: 5:36:47, time: 47.1525
it: 38900/80000, lr: 0.005554, loss: 3.1056, eta: 5:35:55, time: 46.0018
it: 39000/80000, lr: 0.005542, loss: 3.1660, eta: 5:35:03, time: 46.8652
it: 39100/80000, lr: 0.005530, loss: 3.0924, eta: 5:34:23, time: 56.9341
it: 39200/80000, lr: 0.005517, loss: 3.1278, eta: 5:33:35, time: 50.7408
it: 39300/80000, lr: 0.005505, loss: 3.1072, eta: 5:32:46, time: 49.1881
it: 39400/80000, lr: 0.005493, loss: 3.1572, eta: 5:31:59, time: 50.1057
it: 39500/80000, lr: 0.005481, loss: 3.1344, eta: 5:31:22, time: 61.4705
it: 39600/80000, lr: 0.005469, loss: 3.1510, eta: 5:30:32, time: 48.3469
it: 39700/80000, lr: 0.005457, loss: 3.0749, eta: 5:29:44, time: 50.0968
it: 39800/80000, lr: 0.005444, loss: 3.1305, eta: 5:28:52, time: 46.3336
it: 39900/80000, lr: 0.005432, loss: 3.1497, eta: 5:28:12, time: 58.0984
it: 40000/80000, lr: 0.005420, loss: 3.1403, eta: 5:27:22, time: 47.6030
it: 40100/80000, lr: 0.005408, loss: 3.1868, eta: 5:26:30, time: 46.4161
it: 40200/80000, lr: 0.005396, loss: 3.2053, eta: 5:25:49, time: 57.7842
it: 40300/80000, lr: 0.005383, loss: 3.1199, eta: 5:24:59, time: 47.3195
it: 40400/80000, lr: 0.005371, loss: 3.1367, eta: 5:24:07, time: 46.6537
it: 40500/80000, lr: 0.005359, loss: 3.1101, eta: 5:23:20, time: 51.5118
it: 40600/80000, lr: 0.005347, loss: 3.1538, eta: 5:22:43, time: 61.1764
it: 40700/80000, lr: 0.005335, loss: 3.1037, eta: 5:22:07, time: 63.0115
it: 40800/80000, lr: 0.005322, loss: 3.0768, eta: 5:21:18, time: 49.4252
it: 40900/80000, lr: 0.005310, loss: 3.1206, eta: 5:20:26, time: 46.5047
it: 41000/80000, lr: 0.005298, loss: 3.0431, eta: 5:20:11, time: 84.5670
it: 41100/80000, lr: 0.005286, loss: 3.1266, eta: 5:19:18, time: 45.6522
it: 41200/80000, lr: 0.005273, loss: 3.1103, eta: 5:18:29, time: 49.4475
it: 41300/80000, lr: 0.005261, loss: 3.2854, eta: 5:17:51, time: 60.8287
it: 41400/80000, lr: 0.005249, loss: 3.2542, eta: 5:17:01, time: 48.7325
it: 41500/80000, lr: 0.005237, loss: 3.1535, eta: 5:16:09, time: 46.7609
it: 41600/80000, lr: 0.005224, loss: 3.1062, eta: 5:15:20, time: 48.9787
it: 41700/80000, lr: 0.005212, loss: 3.1607, eta: 5:14:42, time: 61.4455
it: 41800/80000, lr: 0.005200, loss: 3.0653, eta: 5:13:50, time: 46.4791
it: 41900/80000, lr: 0.005188, loss: 3.0530, eta: 5:13:02, time: 51.0434
it: 42000/80000, lr: 0.005175, loss: 3.1968, eta: 5:12:11, time: 47.2849
it: 42100/80000, lr: 0.005163, loss: 3.1136, eta: 5:11:32, time: 61.1059
it: 42200/80000, lr: 0.005151, loss: 3.0930, eta: 5:10:42, time: 48.2100
it: 42300/80000, lr: 0.005139, loss: 3.0987, eta: 5:09:52, time: 48.7559
it: 42400/80000, lr: 0.005126, loss: 3.2043, eta: 5:09:07, time: 53.7499
it: 42500/80000, lr: 0.005114, loss: 3.1233, eta: 5:08:38, time: 72.9861
it: 42600/80000, lr: 0.005102, loss: 3.1865, eta: 5:07:46, time: 46.2639
it: 42700/80000, lr: 0.005090, loss: 3.1952, eta: 5:06:58, time: 50.7350
it: 42800/80000, lr: 0.005077, loss: 3.1662, eta: 5:06:32, time: 76.6306
it: 42900/80000, lr: 0.005065, loss: 3.1546, eta: 5:05:56, time: 64.5806
it: 43000/80000, lr: 0.005053, loss: 3.1142, eta: 5:05:11, time: 54.7412
it: 43100/80000, lr: 0.005040, loss: 3.0916, eta: 5:04:26, time: 54.3560
it: 43200/80000, lr: 0.005028, loss: 3.0580, eta: 5:03:52, time: 68.1069
it: 43300/80000, lr: 0.005016, loss: 3.1641, eta: 5:03:10, time: 58.7916
it: 43400/80000, lr: 0.005004, loss: 3.0627, eta: 5:02:23, time: 52.4902
it: 43500/80000, lr: 0.004991, loss: 3.1605, eta: 5:01:36, time: 52.0490
it: 43600/80000, lr: 0.004979, loss: 3.1134, eta: 5:00:54, time: 58.8605
it: 43700/80000, lr: 0.004967, loss: 3.1512, eta: 5:00:08, time: 53.3952
it: 43800/80000, lr: 0.004954, loss: 3.1290, eta: 4:59:19, time: 50.5003
it: 43900/80000, lr: 0.004942, loss: 3.0725, eta: 4:58:43, time: 66.6733
it: 44000/80000, lr: 0.004930, loss: 3.1460, eta: 4:57:56, time: 52.3955
it: 44100/80000, lr: 0.004917, loss: 3.1312, eta: 4:57:11, time: 55.9555
it: 44200/80000, lr: 0.004905, loss: 3.1338, eta: 4:56:22, time: 49.6279
it: 44300/80000, lr: 0.004893, loss: 3.0838, eta: 4:55:46, time: 67.3223
it: 44400/80000, lr: 0.004880, loss: 3.1255, eta: 4:55:09, time: 64.9426
it: 44500/80000, lr: 0.004868, loss: 3.0429, eta: 4:54:35, time: 70.5481
it: 44600/80000, lr: 0.004856, loss: 3.1387, eta: 4:54:05, time: 74.1768
it: 44700/80000, lr: 0.004843, loss: 3.0527, eta: 4:53:37, time: 77.4008
it: 44800/80000, lr: 0.004831, loss: 3.1742, eta: 4:52:53, time: 57.3893
it: 44900/80000, lr: 0.004819, loss: 3.1801, eta: 4:52:09, time: 57.7785
it: 45000/80000, lr: 0.004806, loss: 3.1267, eta: 4:51:25, time: 56.8860
it: 45100/80000, lr: 0.004794, loss: 3.1762, eta: 4:50:55, time: 76.8974
it: 45200/80000, lr: 0.004782, loss: 3.0821, eta: 4:50:17, time: 64.5237
it: 45300/80000, lr: 0.004769, loss: 3.1799, eta: 4:49:37, time: 64.1532
it: 45400/80000, lr: 0.004757, loss: 3.1385, eta: 4:49:12, time: 81.9414
it: 45500/80000, lr: 0.004744, loss: 3.0186, eta: 4:48:26, time: 56.0474
it: 45600/80000, lr: 0.004732, loss: 3.1003, eta: 4:47:35, time: 49.3858
it: 45700/80000, lr: 0.004720, loss: 3.0249, eta: 4:46:46, time: 51.9261
it: 45800/80000, lr: 0.004707, loss: 3.0686, eta: 4:46:06, time: 63.9527
it: 45900/80000, lr: 0.004695, loss: 3.0372, eta: 4:45:19, time: 54.1423
it: 46000/80000, lr: 0.004683, loss: 3.0649, eta: 4:44:39, time: 63.1340
it: 46100/80000, lr: 0.004670, loss: 3.0945, eta: 4:43:57, time: 61.9731
it: 46200/80000, lr: 0.004658, loss: 3.0647, eta: 4:43:23, time: 71.9732
it: 46300/80000, lr: 0.004645, loss: 3.0454, eta: 4:42:37, time: 56.1725
it: 46400/80000, lr: 0.004633, loss: 3.0396, eta: 4:41:51, time: 56.2976
it: 46500/80000, lr: 0.004620, loss: 3.0562, eta: 4:41:04, time: 55.2961
it: 46600/80000, lr: 0.004608, loss: 3.1330, eta: 4:40:28, time: 70.1888
it: 46700/80000, lr: 0.004596, loss: 3.0690, eta: 4:39:39, time: 53.3815
it: 46800/80000, lr: 0.004583, loss: 3.0871, eta: 4:38:52, time: 54.6896
it: 46900/80000, lr: 0.004571, loss: 3.0543, eta: 4:38:14, time: 67.0859
it: 47000/80000, lr: 0.004558, loss: 2.9731, eta: 4:37:24, time: 52.0190
it: 47100/80000, lr: 0.004546, loss: 3.0938, eta: 4:36:36, time: 53.6202
it: 47200/80000, lr: 0.004534, loss: 3.0862, eta: 4:35:47, time: 52.1235
it: 47300/80000, lr: 0.004521, loss: 3.1081, eta: 4:35:05, time: 63.8005
it: 47400/80000, lr: 0.004509, loss: 3.0426, eta: 4:34:16, time: 51.3464
it: 47500/80000, lr: 0.004496, loss: 3.0748, eta: 4:33:27, time: 53.0240
it: 47600/80000, lr: 0.004484, loss: 3.0369, eta: 4:32:37, time: 51.3227
it: 47700/80000, lr: 0.004471, loss: 3.0797, eta: 4:31:54, time: 61.9180
it: 47800/80000, lr: 0.004459, loss: 3.1187, eta: 4:31:05, time: 52.0282
it: 47900/80000, lr: 0.004446, loss: 3.1701, eta: 4:30:13, time: 48.9542
it: 48000/80000, lr: 0.004434, loss: 3.0585, eta: 4:29:31, time: 62.9848
it: 48100/80000, lr: 0.004421, loss: 3.0993, eta: 4:28:43, time: 54.6866
it: 48200/80000, lr: 0.004409, loss: 3.0738, eta: 4:27:54, time: 52.5182
it: 48300/80000, lr: 0.004396, loss: 3.0214, eta: 4:27:05, time: 52.5409
it: 48400/80000, lr: 0.004384, loss: 3.1300, eta: 4:26:22, time: 62.5602
it: 48500/80000, lr: 0.004371, loss: 3.0568, eta: 4:25:33, time: 52.3981
it: 48600/80000, lr: 0.004359, loss: 3.0150, eta: 4:24:43, time: 52.5108
it: 48700/80000, lr: 0.004346, loss: 3.1568, eta: 4:23:54, time: 52.8048
it: 48800/80000, lr: 0.004334, loss: 3.1345, eta: 4:23:12, time: 63.0346
it: 48900/80000, lr: 0.004321, loss: 3.1208, eta: 4:22:22, time: 52.6468
it: 49000/80000, lr: 0.004309, loss: 3.0682, eta: 4:21:34, time: 54.7606
it: 49100/80000, lr: 0.004296, loss: 3.0562, eta: 4:20:43, time: 50.1955
it: 49200/80000, lr: 0.004284, loss: 3.0689, eta: 4:20:01, time: 63.9088
it: 49300/80000, lr: 0.004271, loss: 3.0713, eta: 4:19:10, time: 50.1101
it: 49400/80000, lr: 0.004259, loss: 3.1005, eta: 4:18:20, time: 51.6200
it: 49500/80000, lr: 0.004246, loss: 2.9671, eta: 4:17:34, time: 58.4250
it: 49600/80000, lr: 0.004234, loss: 3.0851, eta: 4:16:44, time: 51.5288
it: 49700/80000, lr: 0.004221, loss: 3.0584, eta: 4:15:53, time: 49.7649
it: 49800/80000, lr: 0.004209, loss: 3.0920, eta: 4:15:09, time: 61.5086
it: 49900/80000, lr: 0.004196, loss: 2.9891, eta: 4:14:29, time: 68.5389
it: 50000/80000, lr: 0.004184, loss: 3.0139, eta: 4:13:42, time: 57.7435
it: 50100/80000, lr: 0.004171, loss: 3.0481, eta: 4:12:55, time: 56.7019
it: 50200/80000, lr: 0.004159, loss: 3.0329, eta: 4:12:05, time: 52.8387
it: 50300/80000, lr: 0.004146, loss: 3.0086, eta: 4:11:25, time: 68.7110
it: 50400/80000, lr: 0.004133, loss: 3.0521, eta: 4:10:43, time: 65.3682
it: 50500/80000, lr: 0.004121, loss: 3.0457, eta: 4:09:59, time: 62.5852
it: 50600/80000, lr: 0.004108, loss: 3.0282, eta: 4:09:15, time: 62.8655
it: 50700/80000, lr: 0.004096, loss: 3.0737, eta: 4:08:29, time: 58.4100
it: 50800/80000, lr: 0.004083, loss: 3.1406, eta: 4:07:39, time: 52.3634
it: 50900/80000, lr: 0.004071, loss: 3.0472, eta: 4:06:50, time: 53.9590
it: 51000/80000, lr: 0.004058, loss: 3.0611, eta: 4:06:06, time: 63.6990
it: 51100/80000, lr: 0.004045, loss: 3.0443, eta: 4:05:16, time: 52.6623
it: 51200/80000, lr: 0.004033, loss: 2.9939, eta: 4:04:26, time: 52.0514
it: 51300/80000, lr: 0.004020, loss: 3.0071, eta: 4:03:36, time: 52.9309
it: 51400/80000, lr: 0.004008, loss: 3.0373, eta: 4:02:53, time: 64.9024
it: 51500/80000, lr: 0.003995, loss: 3.1183, eta: 4:02:03, time: 53.0663
it: 51600/80000, lr: 0.003982, loss: 3.0281, eta: 4:01:14, time: 54.1484
it: 51700/80000, lr: 0.003970, loss: 3.0590, eta: 4:00:23, time: 50.9916
it: 51800/80000, lr: 0.003957, loss: 3.1544, eta: 3:59:43, time: 71.3560
it: 51900/80000, lr: 0.003944, loss: 3.0052, eta: 3:58:56, time: 58.8237
it: 52000/80000, lr: 0.003932, loss: 3.0825, eta: 3:58:05, time: 50.3528
it: 52100/80000, lr: 0.003919, loss: 3.0188, eta: 3:57:25, time: 72.4338
it: 52200/80000, lr: 0.003907, loss: 3.0839, eta: 3:56:38, time: 58.9421
it: 52300/80000, lr: 0.003894, loss: 3.0602, eta: 3:55:47, time: 50.8445
it: 52400/80000, lr: 0.003881, loss: 3.0233, eta: 3:55:02, time: 62.1890
it: 52500/80000, lr: 0.003869, loss: 3.0403, eta: 3:54:28, time: 83.2657
it: 52600/80000, lr: 0.003856, loss: 2.9450, eta: 3:54:00, time: 96.7355
it: 52700/80000, lr: 0.003843, loss: 3.0562, eta: 3:53:19, time: 70.0584
it: 52800/80000, lr: 0.003831, loss: 3.0591, eta: 3:52:29, time: 54.8172
it: 52900/80000, lr: 0.003818, loss: 3.0213, eta: 3:51:51, time: 75.9656
it: 53000/80000, lr: 0.003805, loss: 3.0842, eta: 3:51:07, time: 65.3919
it: 53100/80000, lr: 0.003793, loss: 3.0036, eta: 3:50:23, time: 67.1696
it: 53200/80000, lr: 0.003780, loss: 3.1019, eta: 3:49:43, time: 73.3586
it: 53300/80000, lr: 0.003767, loss: 3.0327, eta: 3:49:01, time: 69.6502
it: 53400/80000, lr: 0.003754, loss: 2.9788, eta: 3:48:18, time: 68.3866
it: 53500/80000, lr: 0.003742, loss: 3.0026, eta: 3:47:40, time: 78.8659
it: 53600/80000, lr: 0.003729, loss: 3.0458, eta: 3:46:58, time: 71.6249
it: 53700/80000, lr: 0.003716, loss: 3.0110, eta: 3:46:13, time: 64.7878
it: 53800/80000, lr: 0.003704, loss: 3.0661, eta: 3:45:26, time: 61.1546
it: 53900/80000, lr: 0.003691, loss: 3.0380, eta: 3:44:36, time: 55.6653
it: 54000/80000, lr: 0.003678, loss: 3.0562, eta: 3:43:57, time: 76.9294
it: 54100/80000, lr: 0.003665, loss: 3.0765, eta: 3:43:12, time: 66.2602
it: 54200/80000, lr: 0.003653, loss: 3.0268, eta: 3:42:26, time: 64.2629
it: 54300/80000, lr: 0.003640, loss: 3.0564, eta: 3:41:39, time: 61.6205
it: 54400/80000, lr: 0.003627, loss: 3.0028, eta: 3:41:00, time: 77.0488
it: 54500/80000, lr: 0.003614, loss: 2.9938, eta: 3:40:11, time: 59.6423
it: 54600/80000, lr: 0.003602, loss: 3.0092, eta: 3:39:30, time: 74.3005
it: 54700/80000, lr: 0.003589, loss: 3.0750, eta: 3:38:53, time: 83.9242
it: 54800/80000, lr: 0.003576, loss: 2.9773, eta: 3:38:26, time: 106.0738
it: 54900/80000, lr: 0.003563, loss: 2.9914, eta: 3:37:54, time: 95.5871
it: 55000/80000, lr: 0.003551, loss: 3.0124, eta: 3:37:20, time: 91.0455
it: 55100/80000, lr: 0.003538, loss: 2.9579, eta: 3:36:49, time: 100.2232
it: 55200/80000, lr: 0.003525, loss: 3.1040, eta: 3:36:12, time: 86.3297
it: 55300/80000, lr: 0.003512, loss: 3.0148, eta: 3:35:33, time: 81.4949
it: 55400/80000, lr: 0.003499, loss: 2.9840, eta: 3:34:52, time: 78.3937
it: 55500/80000, lr: 0.003487, loss: 3.0563, eta: 3:34:23, time: 104.4723
it: 55600/80000, lr: 0.003474, loss: 3.0255, eta: 3:33:50, time: 98.4805
it: 55700/80000, lr: 0.003461, loss: 2.9765, eta: 3:33:17, time: 96.4483
it: 55800/80000, lr: 0.003448, loss: 3.0156, eta: 3:32:35, time: 78.3361
it: 55900/80000, lr: 0.003435, loss: 3.0259, eta: 3:32:15, time: 126.8279
it: 56000/80000, lr: 0.003422, loss: 3.0163, eta: 3:31:37, time: 88.1780
it: 56100/80000, lr: 0.003410, loss: 2.9785, eta: 3:31:00, time: 89.6108
it: 56200/80000, lr: 0.003397, loss: 3.0442, eta: 3:30:21, time: 87.5427
it: 56300/80000, lr: 0.003384, loss: 3.0306, eta: 3:29:51, time: 107.1085
it: 56400/80000, lr: 0.003371, loss: 2.9994, eta: 3:29:13, time: 89.4306
it: 56500/80000, lr: 0.003358, loss: 2.9561, eta: 3:28:35, time: 88.7709
it: 56600/80000, lr: 0.003345, loss: 2.9846, eta: 3:28:03, time: 105.2679
it: 56700/80000, lr: 0.003333, loss: 3.0466, eta: 3:27:18, time: 74.4675
it: 56800/80000, lr: 0.003320, loss: 2.9499, eta: 3:26:31, time: 67.6624
it: 56900/80000, lr: 0.003307, loss: 3.0238, eta: 3:25:41, time: 61.2511
it: 57000/80000, lr: 0.003294, loss: 2.9096, eta: 3:25:04, time: 95.1498
it: 57100/80000, lr: 0.003281, loss: 3.0027, eta: 3:24:22, time: 81.8946
it: 57200/80000, lr: 0.003268, loss: 2.9679, eta: 3:23:44, time: 91.9413
it: 57300/80000, lr: 0.003255, loss: 3.0084, eta: 3:23:04, time: 88.5748
it: 57400/80000, lr: 0.003242, loss: 3.0078, eta: 3:22:18, time: 74.2970
it: 57500/80000, lr: 0.003229, loss: 2.9446, eta: 3:21:35, time: 78.8613
it: 57600/80000, lr: 0.003216, loss: 3.0160, eta: 3:20:50, time: 78.3513
it: 57700/80000, lr: 0.003204, loss: 3.0032, eta: 3:20:08, time: 84.3579
it: 57800/80000, lr: 0.003191, loss: 3.0192, eta: 3:19:31, time: 96.4490
it: 57900/80000, lr: 0.003178, loss: 2.9304, eta: 3:18:54, time: 99.8829
it: 58000/80000, lr: 0.003165, loss: 2.9531, eta: 3:18:16, time: 95.2129
it: 58100/80000, lr: 0.003152, loss: 2.9505, eta: 3:17:43, time: 109.7133
it: 58200/80000, lr: 0.003139, loss: 3.0094, eta: 3:17:07, time: 102.5922
it: 58300/80000, lr: 0.003126, loss: 2.9810, eta: 3:16:30, time: 100.6497
it: 58400/80000, lr: 0.003113, loss: 2.9971, eta: 3:15:38, time: 60.9385
it: 58500/80000, lr: 0.003100, loss: 2.9932, eta: 3:15:08, time: 121.5401
it: 58600/80000, lr: 0.003087, loss: 2.9309, eta: 3:14:32, time: 102.6857
it: 58700/80000, lr: 0.003074, loss: 2.9298, eta: 3:13:55, time: 103.4185
it: 58800/80000, lr: 0.003061, loss: 2.9329, eta: 3:13:17, time: 101.4700
it: 58900/80000, lr: 0.003048, loss: 2.9435, eta: 3:12:34, time: 87.8133
it: 59000/80000, lr: 0.003035, loss: 2.9035, eta: 3:11:49, time: 82.0736
it: 59100/80000, lr: 0.003022, loss: 2.9805, eta: 3:10:57, time: 63.3444
it: 59200/80000, lr: 0.003009, loss: 2.9708, eta: 3:10:15, time: 90.8319
it: 59300/80000, lr: 0.002996, loss: 2.9602, eta: 3:09:37, time: 102.8824
it: 59400/80000, lr: 0.002983, loss: 2.9875, eta: 3:08:44, time: 61.2566
it: 59500/80000, lr: 0.002970, loss: 2.9430, eta: 3:07:47, time: 49.0055
it: 59600/80000, lr: 0.002957, loss: 2.9977, eta: 3:06:57, time: 67.0627
it: 59700/80000, lr: 0.002944, loss: 2.9335, eta: 3:06:06, time: 67.9295
it: 59800/80000, lr: 0.002931, loss: 2.9917, eta: 3:05:25, time: 95.4909
it: 59900/80000, lr: 0.002918, loss: 2.9901, eta: 3:04:43, time: 96.4286
it: 60000/80000, lr: 0.002905, loss: 2.9250, eta: 3:04:10, time: 119.2944
it: 60100/80000, lr: 0.002891, loss: 2.9212, eta: 3:03:33, time: 110.3180
it: 60200/80000, lr: 0.002878, loss: 3.0018, eta: 3:03:00, time: 123.0677
it: 60300/80000, lr: 0.002865, loss: 2.9974, eta: 3:02:25, time: 118.4177
it: 60400/80000, lr: 0.002852, loss: 2.9597, eta: 3:01:52, time: 127.4223
it: 60500/80000, lr: 0.002839, loss: 2.9527, eta: 3:01:18, time: 120.9701
it: 60600/80000, lr: 0.002826, loss: 2.9878, eta: 3:00:44, time: 123.8147
it: 60700/80000, lr: 0.002813, loss: 2.8942, eta: 2:59:58, time: 88.4044
it: 60800/80000, lr: 0.002800, loss: 2.9286, eta: 2:59:13, time: 89.1005
it: 60900/80000, lr: 0.002787, loss: 2.9715, eta: 2:58:29, time: 95.4528
it: 61000/80000, lr: 0.002774, loss: 3.1204, eta: 2:57:48, time: 102.5716
it: 61100/80000, lr: 0.002760, loss: 3.0437, eta: 2:57:16, time: 135.4738
it: 61200/80000, lr: 0.002747, loss: 3.0677, eta: 2:56:40, time: 123.3590
it: 61300/80000, lr: 0.002734, loss: 2.9750, eta: 2:56:02, time: 116.5434
it: 61400/80000, lr: 0.002721, loss: 2.9781, eta: 2:55:20, time: 103.2466
it: 61500/80000, lr: 0.002708, loss: 2.9674, eta: 2:54:30, time: 77.0288
it: 61600/80000, lr: 0.002695, loss: 3.0060, eta: 2:53:44, time: 94.2723
it: 61700/80000, lr: 0.002681, loss: 2.9928, eta: 2:52:49, time: 63.0657
it: 61800/80000, lr: 0.002668, loss: 2.9704, eta: 2:52:00, time: 80.2549
it: 61900/80000, lr: 0.002655, loss: 3.0741, eta: 2:51:21, time: 116.3921
it: 62000/80000, lr: 0.002642, loss: 2.9774, eta: 2:50:35, time: 94.2777
it: 62100/80000, lr: 0.002629, loss: 3.0001, eta: 2:49:55, time: 116.4985
it: 62200/80000, lr: 0.002615, loss: 2.9969, eta: 2:49:19, time: 129.0918
it: 62300/80000, lr: 0.002602, loss: 2.9807, eta: 2:48:39, time: 118.9629
it: 62400/80000, lr: 0.002589, loss: 2.9787, eta: 2:48:00, time: 122.2084
it: 62500/80000, lr: 0.002576, loss: 2.9768, eta: 2:47:23, time: 127.3778
it: 62600/80000, lr: 0.002562, loss: 2.9391, eta: 2:46:46, time: 130.2322
it: 62700/80000, lr: 0.002549, loss: 2.9631, eta: 2:46:12, time: 144.3434
it: 62800/80000, lr: 0.002536, loss: 2.9507, eta: 2:45:35, time: 131.5645
it: 62900/80000, lr: 0.002523, loss: 2.9430, eta: 2:45:00, time: 144.1122
it: 63000/80000, lr: 0.002509, loss: 2.9556, eta: 2:44:21, time: 128.5453
it: 63100/80000, lr: 0.002496, loss: 3.0049, eta: 2:43:40, time: 121.3453
it: 63200/80000, lr: 0.002483, loss: 2.9705, eta: 2:43:08, time: 154.8554
it: 63300/80000, lr: 0.002469, loss: 2.9667, eta: 2:42:31, time: 138.6007
it: 63400/80000, lr: 0.002456, loss: 2.9776, eta: 2:41:53, time: 137.4134
it: 63500/80000, lr: 0.002443, loss: 3.0192, eta: 2:41:16, time: 141.7864
it: 63600/80000, lr: 0.002430, loss: 2.9909, eta: 2:40:27, time: 95.7517
it: 63700/80000, lr: 0.002416, loss: 2.9408, eta: 2:39:48, time: 134.5456
it: 63800/80000, lr: 0.002403, loss: 2.9740, eta: 2:39:04, time: 118.6319
it: 63900/80000, lr: 0.002389, loss: 2.9254, eta: 2:38:20, time: 116.0084
it: 64000/80000, lr: 0.002376, loss: 3.0333, eta: 2:37:38, time: 127.4698
it: 64100/80000, lr: 0.002363, loss: 2.9645, eta: 2:36:50, time: 103.7667
it: 64200/80000, lr: 0.002349, loss: 2.9192, eta: 2:36:08, time: 131.0114
it: 64300/80000, lr: 0.002336, loss: 2.8736, eta: 2:35:28, time: 136.1015
it: 64400/80000, lr: 0.002323, loss: 2.9600, eta: 2:34:44, time: 123.5751
it: 64500/80000, lr: 0.002309, loss: 2.9351, eta: 2:33:55, time: 104.0034
it: 64600/80000, lr: 0.002296, loss: 3.0594, eta: 2:33:02, time: 84.8331
it: 64700/80000, lr: 0.002282, loss: 2.9665, eta: 2:32:15, time: 112.6103
it: 64800/80000, lr: 0.002269, loss: 2.9918, eta: 2:31:30, time: 125.6463
it: 64900/80000, lr: 0.002255, loss: 2.9283, eta: 2:30:44, time: 117.6904
it: 65000/80000, lr: 0.002242, loss: 2.9409, eta: 2:29:55, time: 108.1174
it: 65100/80000, lr: 0.002229, loss: 2.9069, eta: 2:29:01, time: 86.2213
it: 65200/80000, lr: 0.002215, loss: 2.9613, eta: 2:28:13, time: 111.8541
it: 65300/80000, lr: 0.002202, loss: 2.9100, eta: 2:27:23, time: 106.5348
it: 65400/80000, lr: 0.002188, loss: 2.9031, eta: 2:26:32, time: 99.0689
it: 65500/80000, lr: 0.002175, loss: 2.9301, eta: 2:25:44, time: 117.5283
it: 65600/80000, lr: 0.002161, loss: 2.9372, eta: 2:24:54, time: 105.2386
it: 65700/80000, lr: 0.002148, loss: 2.9921, eta: 2:24:03, time: 103.1019
it: 65800/80000, lr: 0.002134, loss: 2.9556, eta: 2:23:12, time: 102.5304
it: 65900/80000, lr: 0.002121, loss: 2.9544, eta: 2:22:18, time: 91.0475
it: 66000/80000, lr: 0.002107, loss: 2.9168, eta: 2:21:26, time: 103.6635
it: 66100/80000, lr: 0.002094, loss: 2.9093, eta: 2:20:35, time: 106.0424
it: 66200/80000, lr: 0.002080, loss: 2.9807, eta: 2:19:43, time: 102.9351
it: 66300/80000, lr: 0.002066, loss: 2.9183, eta: 2:18:55, time: 122.4699
it: 66400/80000, lr: 0.002053, loss: 2.9280, eta: 2:18:04, time: 107.7975
it: 66500/80000, lr: 0.002039, loss: 2.9326, eta: 2:17:09, time: 89.9468
it: 66600/80000, lr: 0.002026, loss: 3.0528, eta: 2:16:20, time: 119.1363
it: 66700/80000, lr: 0.002012, loss: 2.9884, eta: 2:15:26, time: 98.8093
it: 66800/80000, lr: 0.001998, loss: 2.9408, eta: 2:14:34, time: 103.7995
it: 66900/80000, lr: 0.001985, loss: 2.9376, eta: 2:13:41, time: 103.0072
it: 67000/80000, lr: 0.001971, loss: 2.9480, eta: 2:12:46, time: 97.1165
it: 67100/80000, lr: 0.001957, loss: 2.8789, eta: 2:11:55, time: 115.3874
it: 67200/80000, lr: 0.001944, loss: 2.9090, eta: 2:11:05, time: 117.9295
it: 67300/80000, lr: 0.001930, loss: 2.8754, eta: 2:10:13, time: 113.9400
it: 67400/80000, lr: 0.001916, loss: 2.9187, eta: 2:09:22, time: 115.1403
it: 67500/80000, lr: 0.001903, loss: 2.9258, eta: 2:08:27, time: 98.6296
it: 67600/80000, lr: 0.001889, loss: 2.9329, eta: 2:07:33, time: 100.7547
it: 67700/80000, lr: 0.001875, loss: 2.9210, eta: 2:06:38, time: 100.3379
it: 67800/80000, lr: 0.001862, loss: 2.9424, eta: 2:05:50, time: 135.8394
it: 67900/80000, lr: 0.001848, loss: 2.9500, eta: 2:04:57, time: 113.8511
it: 68000/80000, lr: 0.001834, loss: 2.9540, eta: 2:04:04, time: 110.5139
it: 68100/80000, lr: 0.001820, loss: 2.9620, eta: 2:03:10, time: 110.6720
it: 68200/80000, lr: 0.001807, loss: 2.8993, eta: 2:02:16, time: 111.6215
it: 68300/80000, lr: 0.001793, loss: 3.0017, eta: 2:01:24, time: 119.9483
it: 68400/80000, lr: 0.001779, loss: 2.8830, eta: 2:00:34, time: 133.2009
it: 68500/80000, lr: 0.001765, loss: 2.9374, eta: 1:59:44, time: 135.2409
it: 68600/80000, lr: 0.001751, loss: 2.9291, eta: 1:58:47, time: 93.5648
it: 68700/80000, lr: 0.001738, loss: 2.9458, eta: 1:57:53, time: 117.0809
it: 68800/80000, lr: 0.001724, loss: 2.9460, eta: 1:56:58, time: 109.8487
it: 68900/80000, lr: 0.001710, loss: 2.8948, eta: 1:56:04, time: 112.7333
it: 69000/80000, lr: 0.001696, loss: 2.9109, eta: 1:55:07, time: 101.7975
it: 69100/80000, lr: 0.001682, loss: 2.9374, eta: 1:54:07, time: 80.2394
it: 69200/80000, lr: 0.001668, loss: 2.8926, eta: 1:53:13, time: 116.3337
it: 69300/80000, lr: 0.001654, loss: 2.8744, eta: 1:52:18, time: 114.8198
it: 69400/80000, lr: 0.001640, loss: 2.8578, eta: 1:51:24, time: 120.9420
it: 69500/80000, lr: 0.001626, loss: 2.9028, eta: 1:50:28, time: 111.8518
it: 69600/80000, lr: 0.001613, loss: 2.8341, eta: 1:49:32, time: 109.7273
it: 69700/80000, lr: 0.001599, loss: 2.8437, eta: 1:48:35, time: 109.4845
it: 69800/80000, lr: 0.001585, loss: 2.8749, eta: 1:47:33, time: 72.3883
it: 69900/80000, lr: 0.001571, loss: 2.8836, eta: 1:46:30, time: 62.3555
it: 70000/80000, lr: 0.001557, loss: 2.8743, eta: 1:45:28, time: 71.4603
it: 70100/80000, lr: 0.001543, loss: 2.8374, eta: 1:44:24, time: 62.0294
it: 70200/80000, lr: 0.001529, loss: 2.8623, eta: 1:43:21, time: 64.4591
training done, model saved to: ./mv3-leaky/model_70200_2.pth
it: 70300/80000, lr: 0.001515, loss: 2.8381, eta: 1:42:18, time: 59.1868
it: 70400/80000, lr: 0.001500, loss: 2.9009, eta: 1:41:16, time: 75.9720
training done, model saved to: ./mv3-leaky/model_70400_2.pth
it: 70500/80000, lr: 0.001486, loss: 2.9419, eta: 1:40:12, time: 57.4174
it: 70600/80000, lr: 0.001472, loss: 2.8585, eta: 1:39:09, time: 64.5386
training done, model saved to: ./mv3-leaky/model_70600_2.pth
it: 70700/80000, lr: 0.001458, loss: 2.8736, eta: 1:38:06, time: 69.3577
it: 70800/80000, lr: 0.001444, loss: 2.8957, eta: 1:37:03, time: 60.2239
training done, model saved to: ./mv3-leaky/model_70800_2.pth
it: 70900/80000, lr: 0.001430, loss: 2.8785, eta: 1:35:59, time: 60.5948
it: 71000/80000, lr: 0.001416, loss: 2.8872, eta: 1:34:55, time: 58.3199
training done, model saved to: ./mv3-leaky/model_71000_2.pth
it: 71100/80000, lr: 0.001402, loss: 2.8283, eta: 1:33:53, time: 70.1619
it: 71200/80000, lr: 0.001387, loss: 2.9048, eta: 1:32:49, time: 60.8783
training done, model saved to: ./mv3-leaky/model_71200_2.pth
it: 71300/80000, lr: 0.001373, loss: 2.8511, eta: 1:31:45, time: 59.7258
it: 71400/80000, lr: 0.001359, loss: 2.8906, eta: 1:30:41, time: 55.7594
training done, model saved to: ./mv3-leaky/model_71400_2.pth
it: 71500/80000, lr: 0.001345, loss: 2.8547, eta: 1:29:40, time: 85.1560
it: 71600/80000, lr: 0.001331, loss: 2.9266, eta: 1:28:38, time: 71.1513
training done, model saved to: ./mv3-leaky/model_71600_2.pth
it: 71700/80000, lr: 0.001316, loss: 2.9016, eta: 1:27:35, time: 66.4945
it: 71800/80000, lr: 0.001302, loss: 2.9031, eta: 1:26:34, time: 78.2498
training done, model saved to: ./mv3-leaky/model_71800_2.pth
it: 71900/80000, lr: 0.001288, loss: 2.9137, eta: 1:25:30, time: 65.2169
it: 72000/80000, lr: 0.001273, loss: 2.8718, eta: 1:24:27, time: 60.0142
training done, model saved to: ./mv3-leaky/model_72000_2.pth
it: 72100/80000, lr: 0.001259, loss: 2.8650, eta: 1:23:23, time: 63.5349
it: 72200/80000, lr: 0.001245, loss: 2.8520, eta: 1:22:21, time: 67.5860
training done, model saved to: ./mv3-leaky/model_72200_2.pth
it: 72300/80000, lr: 0.001230, loss: 2.8433, eta: 1:21:16, time: 56.5499
it: 72400/80000, lr: 0.001216, loss: 2.8829, eta: 1:20:13, time: 57.3094
training done, model saved to: ./mv3-leaky/model_72400_2.pth
it: 72500/80000, lr: 0.001202, loss: 2.8749, eta: 1:19:08, time: 55.0937
it: 72600/80000, lr: 0.001187, loss: 2.8626, eta: 1:18:05, time: 64.0031
training done, model saved to: ./mv3-leaky/model_72600_2.pth
it: 72700/80000, lr: 0.001173, loss: 2.8666, eta: 1:17:01, time: 59.5343
it: 72800/80000, lr: 0.001158, loss: 2.8693, eta: 1:15:58, time: 57.9950
training done, model saved to: ./mv3-leaky/model_72800_2.pth
it: 72900/80000, lr: 0.001144, loss: 2.8346, eta: 1:14:53, time: 54.8002
it: 73000/80000, lr: 0.001129, loss: 2.8389, eta: 1:13:51, time: 68.8544
training done, model saved to: ./mv3-leaky/model_73000_2.pth
it: 73100/80000, lr: 0.001115, loss: 2.8353, eta: 1:12:47, time: 58.8148
it: 73200/80000, lr: 0.001100, loss: 2.8161, eta: 1:11:43, time: 56.9217
training done, model saved to: ./mv3-leaky/model_73200_2.pth
it: 73300/80000, lr: 0.001086, loss: 2.9107, eta: 1:10:40, time: 67.3840
it: 73400/80000, lr: 0.001071, loss: 2.8986, eta: 1:09:36, time: 57.7447
training done, model saved to: ./mv3-leaky/model_73400_2.pth
it: 73500/80000, lr: 0.001056, loss: 2.8362, eta: 1:08:33, time: 59.5576
it: 73600/80000, lr: 0.001042, loss: 2.8810, eta: 1:07:29, time: 53.6621
training done, model saved to: ./mv3-leaky/model_73600_2.pth
it: 73700/80000, lr: 0.001027, loss: 2.8671, eta: 1:06:26, time: 70.4958
it: 73800/80000, lr: 0.001012, loss: 2.9175, eta: 1:05:22, time: 56.3649
training done, model saved to: ./mv3-leaky/model_73800_2.pth
it: 73900/80000, lr: 0.000998, loss: 2.8306, eta: 1:04:18, time: 57.3338
it: 74000/80000, lr: 0.000983, loss: 2.8742, eta: 1:03:15, time: 58.1525
training done, model saved to: ./mv3-leaky/model_74000_2.pth
it: 74100/80000, lr: 0.000968, loss: 2.7994, eta: 1:02:12, time: 65.9586
it: 74200/80000, lr: 0.000953, loss: 2.8666, eta: 1:01:08, time: 59.4290
training done, model saved to: ./mv3-leaky/model_74200_2.pth
it: 74300/80000, lr: 0.000939, loss: 2.8517, eta: 1:00:05, time: 58.7559
it: 74400/80000, lr: 0.000924, loss: 2.7970, eta: 0:59:01, time: 54.1835
training done, model saved to: ./mv3-leaky/model_74400_2.pth
it: 74500/80000, lr: 0.000909, loss: 2.7857, eta: 0:57:58, time: 74.9145
it: 74600/80000, lr: 0.000894, loss: 2.7922, eta: 0:56:55, time: 57.3387
training done, model saved to: ./mv3-leaky/model_74600_2.pth
it: 74700/80000, lr: 0.000879, loss: 2.8065, eta: 0:55:51, time: 61.9725
it: 74800/80000, lr: 0.000864, loss: 2.8124, eta: 0:54:48, time: 67.2424
training done, model saved to: ./mv3-leaky/model_74800_2.pth
it: 74900/80000, lr: 0.000849, loss: 2.8252, eta: 0:53:45, time: 58.6222
it: 75000/80000, lr: 0.000834, loss: 2.8654, eta: 0:52:41, time: 58.0408
training done, model saved to: ./mv3-leaky/model_75000_2.pth
it: 75100/80000, lr: 0.000819, loss: 2.7914, eta: 0:51:38, time: 57.3373
it: 75200/80000, lr: 0.000804, loss: 2.7853, eta: 0:50:35, time: 68.4806
training done, model saved to: ./mv3-leaky/model_75200_2.pth
it: 75300/80000, lr: 0.000789, loss: 2.8167, eta: 0:49:31, time: 58.7772
it: 75400/80000, lr: 0.000774, loss: 2.7777, eta: 0:48:28, time: 58.5763
training done, model saved to: ./mv3-leaky/model_75400_2.pth
it: 75500/80000, lr: 0.000759, loss: 2.8330, eta: 0:47:24, time: 53.0363
it: 75600/80000, lr: 0.000744, loss: 2.7869, eta: 0:46:22, time: 81.6547
training done, model saved to: ./mv3-leaky/model_75600_2.pth
it: 75700/80000, lr: 0.000728, loss: 2.8180, eta: 0:45:18, time: 55.9877
it: 75800/80000, lr: 0.000713, loss: 2.8211, eta: 0:44:15, time: 56.1082
training done, model saved to: ./mv3-leaky/model_75800_2.pth
it: 75900/80000, lr: 0.000698, loss: 2.8613, eta: 0:43:12, time: 68.4015
it: 76000/80000, lr: 0.000682, loss: 2.8405, eta: 0:42:08, time: 59.0985
training done, model saved to: ./mv3-leaky/model_76000_2.pth
it: 76100/80000, lr: 0.000667, loss: 2.7642, eta: 0:41:05, time: 58.5322
it: 76200/80000, lr: 0.000652, loss: 2.7817, eta: 0:40:01, time: 55.9407
training done, model saved to: ./mv3-leaky/model_76200_2.pth
it: 76300/80000, lr: 0.000636, loss: 2.8156, eta: 0:38:58, time: 69.0522
it: 76400/80000, lr: 0.000621, loss: 2.7667, eta: 0:37:55, time: 60.8398
training done, model saved to: ./mv3-leaky/model_76400_2.pth
it: 76500/80000, lr: 0.000605, loss: 2.8168, eta: 0:36:52, time: 61.3994
it: 76600/80000, lr: 0.000590, loss: 2.7872, eta: 0:35:48, time: 60.1419
training done, model saved to: ./mv3-leaky/model_76600_2.pth
it: 76700/80000, lr: 0.000574, loss: 2.7805, eta: 0:34:45, time: 66.4204
it: 76800/80000, lr: 0.000558, loss: 2.8044, eta: 0:33:42, time: 57.7536
training done, model saved to: ./mv3-leaky/model_76800_2.pth
it: 76900/80000, lr: 0.000543, loss: 2.8195, eta: 0:32:39, time: 58.0389
it: 77000/80000, lr: 0.000527, loss: 2.8087, eta: 0:31:35, time: 55.8034
training done, model saved to: ./mv3-leaky/model_77000_2.pth
it: 77100/80000, lr: 0.000511, loss: 2.8392, eta: 0:30:32, time: 70.1782
it: 77200/80000, lr: 0.000495, loss: 2.7921, eta: 0:29:29, time: 56.4406
training done, model saved to: ./mv3-leaky/model_77200_2.pth
it: 77300/80000, lr: 0.000479, loss: 2.8450, eta: 0:28:26, time: 59.6374
it: 77400/80000, lr: 0.000463, loss: 2.7835, eta: 0:27:22, time: 60.7904
training done, model saved to: ./mv3-leaky/model_77400_2.pth
it: 77500/80000, lr: 0.000447, loss: 2.8151, eta: 0:26:19, time: 70.3197
it: 77600/80000, lr: 0.000431, loss: 2.7869, eta: 0:25:16, time: 59.4537
training done, model saved to: ./mv3-leaky/model_77600_2.pth
it: 77700/80000, lr: 0.000415, loss: 2.7581, eta: 0:24:13, time: 58.6925
it: 77800/80000, lr: 0.000399, loss: 2.7622, eta: 0:23:10, time: 66.0226
training done, model saved to: ./mv3-leaky/model_77800_2.pth
it: 77900/80000, lr: 0.000382, loss: 2.8496, eta: 0:22:06, time: 56.7497
it: 78000/80000, lr: 0.000366, loss: 2.7588, eta: 0:21:03, time: 57.3457
training done, model saved to: ./mv3-leaky/model_78000_2.pth
it: 78100/80000, lr: 0.000349, loss: 2.7260, eta: 0:20:00, time: 55.5680
it: 78200/80000, lr: 0.000333, loss: 2.8033, eta: 0:18:57, time: 69.6938
training done, model saved to: ./mv3-leaky/model_78200_2.pth
it: 78300/80000, lr: 0.000316, loss: 2.8621, eta: 0:17:53, time: 57.2400
it: 78400/80000, lr: 0.000299, loss: 2.7603, eta: 0:16:50, time: 58.4900
training done, model saved to: ./mv3-leaky/model_78400_2.pth
it: 78500/80000, lr: 0.000282, loss: 2.7914, eta: 0:15:47, time: 68.9512
it: 78600/80000, lr: 0.000265, loss: 2.7712, eta: 0:14:44, time: 56.0284
training done, model saved to: ./mv3-leaky/model_78600_2.pth
it: 78700/80000, lr: 0.000248, loss: 2.7181, eta: 0:13:41, time: 56.0299
it: 78800/80000, lr: 0.000231, loss: 2.7515, eta: 0:12:37, time: 56.3082
training done, model saved to: ./mv3-leaky/model_78800_2.pth
it: 78900/80000, lr: 0.000214, loss: 2.7929, eta: 0:11:34, time: 69.4545
it: 79000/80000, lr: 0.000196, loss: 2.7925, eta: 0:10:31, time: 60.0680
training done, model saved to: ./mv3-leaky/model_79000_2.pth
it: 79100/80000, lr: 0.000178, loss: 2.7625, eta: 0:09:28, time: 58.6066
it: 79200/80000, lr: 0.000160, loss: 2.7777, eta: 0:08:25, time: 59.4794
training done, model saved to: ./mv3-leaky/model_79200_2.pth
it: 79300/80000, lr: 0.000142, loss: 2.8014, eta: 0:07:22, time: 64.3057
it: 79400/80000, lr: 0.000124, loss: 2.7128, eta: 0:06:19, time: 65.0225
training done, model saved to: ./mv3-leaky/model_79400_2.pth
it: 79500/80000, lr: 0.000105, loss: 2.7768, eta: 0:05:16, time: 56.2577
it: 79600/80000, lr: 0.000086, loss: 2.7290, eta: 0:04:13, time: 55.5295
training done, model saved to: ./mv3-leaky/model_79600_2.pth
it: 79700/80000, lr: 0.000067, loss: 2.7045, eta: 0:03:09, time: 71.6992
it: 79800/80000, lr: 0.000046, loss: 2.7805, eta: 0:02:06, time: 58.0834
training done, model saved to: ./mv3-leaky/model_79800_2.pth
it: 79900/80000, lr: 0.000025, loss: 2.7420, eta: 0:01:03, time: 58.0783
it: 80000/80000, lr: 0.000000, loss: 2.7430, eta: 0:00:00, time: 66.8612
training done, model saved to: ./mv3-leaky/model_final2.pth
